{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function concates all datasets designated by dataset_paths as a single pandas dataframe.\n",
    "def format_concate(dataset_paths):\n",
    "    datasets = []\n",
    "    for path in dataset_paths:\n",
    "        dataset = pd.read_csv(path, sep='\\t')\n",
    "        datasets.append(dataset)\n",
    "\n",
    "    for i in range(len(datasets)):\n",
    "        dataset = datasets[i]\n",
    "\n",
    "        if 'diag' in dataset.columns:\n",
    "            dataset.rename(columns={'diag': 'diagnosis'}, inplace=True)\n",
    "        elif 'dx' in dataset.columns:\n",
    "            dataset.rename(columns={'dx': 'diagnosis'}, inplace=True)\n",
    "\n",
    "        dataset = dataset[['participant_id', 'diagnosis']]\n",
    "\n",
    "        dataset = dataset.replace(to_replace=[\"CONTROL\", \"No_Known_Disorder\", 0], value=\"0\")\n",
    "        dataset = dataset.replace(to_replace=[\"SCHZ\", \"Schizophrenia_Strict\", 4], value=\"1\")\n",
    "        dataset['participant_id'] = dataset['participant_id'].apply(lambda x: 'sub-' + x if not x.startswith('sub-') else x)\n",
    "\n",
    "        dataset = dataset[(dataset['diagnosis'] == '0') | (dataset['diagnosis'] == '1')]\n",
    "\n",
    "        datasets[i] = dataset\n",
    "\n",
    "    y = pd.concat(datasets, ignore_index=True)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function filters out all participants that do not have a connectivity matrix\n",
    "def filter_data(csv_file_path, y):\n",
    "    csv_df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    csv_participant_ids = csv_df['participant_id'].tolist()\n",
    "\n",
    "    matching_ids = y[y['participant_id'].isin(csv_participant_ids)]\n",
    "\n",
    "    return(matching_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function creates variables x (features), y (diagnosis), and the diagnosis key which is used later. \n",
    "def create_feat(feature_directory, n_parcels, matching_ids):\n",
    "    all_features = []\n",
    "    for participant in matching_ids['participant_id']:\n",
    "        correlation_matrix = np.load(f\"{feature_directory}/z-conn-matrix-{participant}-rest-schaefer{n_parcels}.npy\")\n",
    "        if len(correlation_matrix.shape) == 3:\n",
    "            correlation_matrix = correlation_matrix[0, :, :]\n",
    "        vec_correlation_matrix = nilearn.connectome.sym_matrix_to_vec(correlation_matrix, discard_diagonal=True)\n",
    "        all_features.append(vec_correlation_matrix)\n",
    "\n",
    "    np.savez_compressed(f'correlation_matrix{n_parcels}.npz',a = all_features)\n",
    "\n",
    "    x = np.load(f'correlation_matrix{n_parcels}.npz')['a']\n",
    "    y = matching_ids['diagnosis']\n",
    "    y = y.to_numpy()\n",
    "    diagnosis = pd.DataFrame(y)\n",
    "\n",
    "    y = y.astype('int')\n",
    "\n",
    "    return(x, y, diagnosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes x, y, and diagnosis key and creates training and testing sets for machine learning.\n",
    "def split(x, y, diagnosis):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=0.25, stratify= diagnosis, random_state=99)\n",
    "\n",
    "    y_test = y_test.astype('int')\n",
    "    y_train = y_train.astype('int')\n",
    "\n",
    "    return(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Runs cross validation and outputs fold accuracies, classification report, and the SVM algorithm.\n",
    "def create_svm(x_train, y_train, x_test, y_test):\n",
    "    SVM = svm.SVC(C= 100.0, gamma= 1e-06, kernel= 'sigmoid', probability=True)\n",
    "\n",
    "    cross_pred = cross_val_predict(SVM, x_train, y_train, cv=20, n_jobs=-1)\n",
    "    accuracy = cross_val_score(SVM, x_train, y_train, cv=20, n_jobs=-1)\n",
    "\n",
    "    for i in range(20):\n",
    "        print('Fold {} -- Acc = {}'.format(i, accuracy[i]))\n",
    "\n",
    "    mean = np.mean(accuracy)\n",
    "    print(mean)\n",
    "    \n",
    "    SVM.fit(x_train, y_train)\n",
    "    y_pred = SVM.predict(x_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    return(SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Runs cross validation and outputs fold accuracies, classification report, and the logit algorithm.\n",
    "def create_LR( x_train, y_train, x_test, y_test):\n",
    "    logit = LogisticRegression(multi_class='auto', solver='liblinear')\n",
    "\n",
    "    cross_pred = cross_val_predict(logit, x_train, y_train, cv=20, n_jobs=-1)\n",
    "    acc = cross_val_score(logit, x_train, y_train, cv=20, n_jobs=-1)\n",
    "\n",
    "    for i in range(20):\n",
    "        print('Fold {} -- Acc = {}'.format(i, acc[i]))\n",
    "\n",
    "    mean = np.mean(acc)\n",
    "    print(mean)\n",
    "\n",
    "    logit.fit(x_train, y_train)\n",
    "    y_pred = logit.predict(x_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    return(logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes both SVM & logit algorithms and also training & testing sets and outputs an ROC graph.\n",
    "def roc_graph(svm, logit, x_train, y_train, x_test, y_test, title):\n",
    "    \n",
    "    y_pred_proba_svm = svm.predict_proba(x_test)[::,1]\n",
    "    y_pred_proba_log = logit.predict_proba(x_test)[::,1]\n",
    "\n",
    "    fpr_svm, tpr_svm, _ = metrics.roc_curve(y_test,  y_pred_proba_svm)\n",
    "    auc_svm = metrics.roc_auc_score(y_test, y_pred_proba_svm)\n",
    "    auc_svm = round(auc_svm, 2)\n",
    "\n",
    "    fpr_log, tpr_log, _ = metrics.roc_curve(y_test,  y_pred_proba_log)\n",
    "    auc_log = metrics.roc_auc_score(y_test, y_pred_proba_log)\n",
    "    auc_log = round(auc_log, 2)\n",
    "\n",
    "    plt.plot(fpr_log,tpr_log, label=\"Logit AUC=\"+str(auc_log))\n",
    "    plt.plot(fpr_svm,tpr_svm, label=\"SVM AUC=\"+str(auc_svm))\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", label=\"Chance Level (AUC = 0.5)\")\n",
    "    plt.title(f'{title}')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.legend(loc=4)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
